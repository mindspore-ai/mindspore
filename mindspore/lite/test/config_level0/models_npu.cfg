# tmp close some open source models with similar structure to save time
mobilenet_v1_0.25_128.tflite 2.5
mobilenet_v2_1.0_224.tflite 2.5
squeezenet.tflite 2.5
inception_resnet_v2.tflite 2
inception_v3.tflite 1
inception_v4.tflite 0.5
efficientnet_lite0_fp32_2.tflite 1
deeplabv3_1_default_1.tflite 2.5
6c_seg_nomean_20200610 1.5
ml_video_edit_person_divison 0.5
ml_video_edit_style_transfer_autoportrait.onnx 9
ml_video_edit_style_transfer_candy.onnx 11
ml_video_edit_style_transfer_gongnongbing.onnx 10.5
ml_video_edit_style_transfer_starry.onnx 11
porseg_tmp.onnx;2 1
ml_video_edit_Mnet 1.5
ml_video_edit_hairSeg_have_imageProcessLayer_interpTo145 0.5
ml_video_edit_img_segment 1
ml_video_edit_video_segment_gauss_adaptis_part1 2
ml_video_edit_generate_filter.pb 1
ml_video_edit_generate_filter_pb2tflite.tflite 1
ml_video_edit_img_segment_adaptise.pb;2 0.5
ml_video_edit_video_segment_gauss_adaptis_part2.pb;2 10
ml_video_edit_person_divison_pic 0.5
ml_video_edit_person_divison_video;2 13
ml_video_edit_judge.onnx 5
ml_video_edit_vignet.onnx 0.5
hdc_Face_Aesthetic_MTI_Aesthetic 0.5
hdc_Face_Emotion_MTI_Aesthetic.onnx 32
hdc_Face_Landmark5_MTI_Aesthetic.onnx 0.5
hdc_Image_Aesthetic_MTI_Aesthetic.onnx 0.5
hdc_mobilenet_1w_class.onnx 10
hdc_resnet_1w_class.onnx 5
hdc_age_medium 6
hdc_contour_pose_128 4
hdc_fivembnet 0.5
hdc_mobilenetface 4
hdc_retinaface 6
hdc_resnet 3
ml_video_edit_detect_20211111 1
ml_video_edit_hairSeg_have_imageProcessLayer_interpTo145_20210121 0.5
ml_video_edit_have_imageProcessLayer_interpTo145_20201015 0.5
ml_video_edit_MnetN367_extract_1010_pay 0.5
ml_video_edit_reid 0.5
ml_video_edit_v10_best_model_nomean_20200723 8
# hdc_ocr_attention.onnx 0.5  too many subgraphs when shape fusion is enabled.
# hdc_ocr_detect.onnx 30 #too many subgraphs
ml_edu_kit_hand_detection.onnx 1
ml_edu_kit_hand_key_position.onnx 2
ml_video_edit_oneclick_adaptis.pb;3 2.4
densenet.tflite 3
resnet_v2_101_299.tflite 1
ml_video_edit_enhance.pb 2
ml_video_edit_video_segment_gauss_adaptis_part2_pb2tflite.tflite;2 10
ml_video_edit_img_segment_adaptise_pb2tflite.tflite;2 0.5
#the fifth value of the ml_video_edit_imitate_filter.onnx's output is very small (10-5).
ml_video_edit_imitate_filter.onnx 200
hdc_mobilenet_1w_class.onnx 20
posenet_mobilenet_float_075_1_default_1.tflite 14
nasnet_mobile.tflite 1
# ml_video_edit_art_generate.onnx 0.5  the value overflow (npu runs with fp16) when shape fusion is enabled.
ml_video_edit_art_transfer.onnx;3 3
ml_video_edit_enhance_update_tmp.onnx 0.5
# ml_video_edit_art_generate_20210513.onnx 0.5
# ml_video_edit_art_transfer_20210513.onnx;3 0.5 memory exceed the npu limitation when shape fusion is enabled
ml_video_edit_hair_dyeing_segmodel_v3 0.5
ml_video_edit_makeup_mobilenetv203.onnx 2
ml_video_edit_hairline_segmentation;3 0.5
ml_video_edit_hair_dyeing_migrate_v2.onnx;4 0.5
#not support control flow model with npu
#ml_audio_kit_encoder_v5.pb;6;1:1,32:1,32:1,32:1:1,32
fsr_270_mindspore.pb 1
fsr_360_mindspore.pb 1
fsr_720_mindspore.pb 1
ml_video_edit_hair_dyeing_migrate_v2_fix.onnx;4 1.5
ml_video_edit_seg_320 0.5
hiai_model_0909_kd_rot_ps_softmax.tflite 4
hiai_chinese_english_recognize_model_float32.tflite 3
hiai_bigmodel_ghost_2_1_no_normalized_no_trans_tflite.tflite 2
hiai_bigmodel_ghost_5_1_no_normalized_no_trans_tflite.tflite 3
hiai_cn_recognize_modify_padv2.tflite 5
hiai_model_normalize_object_scene_ps_20200519.tflite 14
mtk_AADB_HADB_MBV2_model_fp32.tflite 1.5
#mtk_AADB_HADB_MBV3_model_fp32.tflite 2.5
mtk_model_ckpt.tflite 5
mtk_age_gender.tflite
mtk_model_face_dress.tflite;1:input
mtk_face_features_v1.tflite 9
mnasnet_1.3_224.tflite;1:input 2
deeplabv3_257_mv_gpu.tflite;1:sub_7 1
multi_person_mobilenet_v1_075_float.tflite;1:sub_2 6
ide_label_base.tflite;1:input 11
#large precision bias error
#ide_label_retrained.tflite;1:input_1
#ml_ei_headpose.tflite;1:input_1
#ml_ei_landmark.tflite;1:input_image
mnist.tflite;1:conv2d_input 1.5
mobilenet.tflite;1:conv2d_input
resnet.tflite;1:input_1
scan_hms_angle1.tflite;1:normalized_input_image_tensor 1.5
scan_hms_detect.tflite;1:normalized_input_image_tensor 41
hiai_latin_ocr.tflite;1:input_0 32
hiai_latin_ocr_1.tflite;1:input_0 5.5
#ml_ocr_jk.tflite;1:input_0
nasnet_mobile.tflite;1:input
#nasnet_large.tflite;1:input
#model_emotions_0727_nosoftmax.tflite;1:input
#ml_ocr_latin.tflite;1:input_0
hiai_PoseEstimation_Pcm.tflite;1:image 12
hiai_ssd_mobilenetv2_object.tflite;1:image_tensor 680
hiai_cv_focusShootOCRModel_02.tflite;1:input_0 6
hiai_cv_poseEstimation.tflite;1:Image 37
mtk_model_normalize_object_scene_ps_20200519_f16.tflite;1:input_0 3
#mtk_age_gender_fp16.tflite;1:img
mtk_model_face_dress_fp16.tflite;1:img
mtk_AADB_HADB_MBV2_model_f16.tflite;1:input_0
#mtk_AADB_HADB_MBV3_model_f16.tflite;1:input_0
#mtk_model_emotions_0725_fp16.tflite;1:input
mtk_face_features_v1_fp16.tflite;1:input 4
#siteAI_digcom_AI_ECN.tflite;1:input_expansion
siteAI_digcom_g2v_keras.tflite;1:conv2d_1_input 2
siteAI_trans_nonlinear.tflite;1:features_placeholder
siteAI_trans_tcpclassify.tflite;1:conv2d_1_input 2.5
siteAI_wireless_depress_w.tflite;1:x-input
siteAI_wireless_restore_w.tflite;1:x-input
magenta_arbitrary-image-stylization-v1-256_fp16_prediction_1.tflite;1:style_image
#ml_object_detect.tflite;1:input/input_data
#ml_object_detect_1.tflite;1:input/input_data
hiai_cpu_face_emotion.tflite;1:input_0 1.5
hiai_cpu_face_gazing.tflite;1:input_0
hiai_cpu_face_headpose.tflite;1:input_0 1.5
hiai_humanDetection.tflite;1:normalized_input_image_tensor 150
hiai_cv_focusShootOCRModel_08.tflite;1:input 4
ml_face_openclose.tflite;1:input
hiai_face_model_npu.tflite;1:input_0 3
hiai_ctpn_feature_map.tflite;1:input_image 2
#hiai_cv_labelDetectorModel_v2's precision deteriorates in HarmonyOS (rom: 336)
hiai_cv_labelDetectorModel_v2.tflite;1:input_0 13
hiai_cv_labelDetectorModel_v4.tflite;1:input_0 1
hiai_dress_detect.tflite;1:data 1
hiai_cv_saliencyDetectorModel.tflite;1:image_tensor
hiai_frozen_inference_graph.tflite;1:image_tensor 2.5
#hiai_ghostnet.tflite;1:input
# hiai_iMaxDN_RGB.tflite;1:input  precision deteriorates in P50 (rom: 511)
hiai_iMaxSR_RGB.tflite;1:input 1
hiai_label_and_video.tflite;1:input_0 4.5
hiai_lm_inference_graph.tflite;1:image_tensor
mnasnet_0.50_224_1_metadata_1.tflite;1:input 3.5
#lite-model_on_device_vision_classifier_popular_us_products_V1_1.tflite;1:uint8_image_input
#lite-model_on_device_vision_classifier_popular_wine_V1_1.tflite;1:uint8_image_input
#lite-model_deeplabv3-mobilenetv2_dm05-float16_1_default_1.tflite;1:sub_7
#lite-model_deeplabv3-mobilenetv2-float16_1_default_1.tflite;1:sub_7
lite-model_east-text-detector_fp16_1.tflite;1:input_images 460
#lite-model_cartoongan_fp16_1.tflite;1:input_photo
lite-model_arbitrary-image-stylization-inceptionv3_fp16_predict_1.tflite;1:style_image 2
gts_detect_5k_tf115.tflite;1:normalized_input_image_tensor
#mtk_new_detect.tflite;1:input
#mtk_model_emotions_0727_nosoftmax.tflite;1:input
mtk_model_normalize_object_scene_ps_20200826_f32_no_softmax.tflite;1:input_0 32
mtk_276landmark_0913.tflite;1:input 4
#mtk_face_recognition.tflite;1:input
#mtk_convert_model.tflite;1:data
smartreply.tflite;1:input_sentence
mindspore_text_classification_tflite.tflite;1:base_input 3
# ml_location.tflite
#ml_text_correction.tflite;1:hed_input
#ml_pic_shopping.tflite;1:images
#ml_vision_guide_detection3_pb2tflite.tflite;1:input/input_data
#ml_vision_guide_detection1_pb2tflite.tflite;1:input/input_data
#ml_pic_shopping_pb2tflite.tflite;1:images
#ml_ocr_jk_pb2tflite.tflite;1:input_0
#ml_ocr_latin_pb2tflite.tflite;1:input_0
scan_hms_angle_pb2tflite.tflite;1:normalized_input_image_tensor 2.5
scan_hms_detect_pb2tflite.tflite;1:normalized_input_image_tensor 110
#ml_location.tflite;1:inputs
ml_face_openclose_tflite.tflite;1:input
#ml_object_detect_pb2tflite.tflite;1:input/input_data
Q_AADB_HADB_MBV2_model.tflite;1:input_0 2.5
#Q_convert.tflite;1:input
#Q_crnn_ori_75w_slim_norm_pb2tflite.tflite;1:input_0
#Q_crnn_ori_v2_405001_notrans_nopre_pb2tflite.tflite;1:input_0
#Q_crnn_screen_slim400w_more_20w_pb2tflite.tflite;1:input_0
Q_dila-small-mix-full-fineturn-390000-nopixel-nosigmoid_tflite.tflite;1:input 2
Q_focusocr_cn_recog.tflite;1:input_0 8
Q_focusocr_jk_recog.tflite;1:input_0 6.5
Q_inception-249970-672-11-16_pb2tflite.tflite;1:input 3
Q_language_model_hrmini_Q4_b4_17w.tflite;1:input_0 51
Q_object_scene.tflite;1:input_0 2.5
#ml_ei_landmark_pb2tflite.tflite;1:input_image
unet_mbv2_05_104pts.tflite;1:input 4.5
hiai_AADB_HADB_MBV2_model_f16.tflite;1:input_0 1
hiai_AADB_HADB_MBV2_model_fp32.tflite;1:input_0 2.5
hiai_detect_curve_model_float32.tflite;1:input
hiai_detectmodel_06_23_960_480_1180700.tflite;1:input 2.5
hiai_detectmodel_desnet_256_128_64_32.tflite;1:input 13
lite-model_aiy_vision_classifier_food_V1_1.tflite;1:input 15
lite-model_disease-classification_1.tflite;1:mobilenetv2_1_00_224_input 30
#lite-model_models_mushroom-identification_v1_1.tflite;1:input
smartreply_1_default_1.tflite;1:input_sentence
#text_classification.tflite;1:embedding_input
Q_detect_fpn_add_inception-1448650.tflite;1:input
Q_hand_0812_pb2tflite.tflite;1:input 8
Q888_age_gender_orderd.tflite;1:input 5.5
Q888_face_dress_mv3y.tflite;1:input
Q888_HADB_AADB_MBV2_model_fp32.tflite;1:input_0 1
Q888_landmark.tflite;1:img
Q888_pose.tflite;1:input 1.5
Q888_lapa158_unet_0924.tflite;1:input 4.5
Q888_isface.tflite;1:data
# Q_hand_0812.pb is not suitable for float16. Out of float16 range.
Q_hand_0812.pb;1:input 8.5
Q_inception-249970-672-11-16.pb;1:input 2.5
Q_dila-small-mix-full-fineturn-390000-nopixel-nosigmoid.pb;1:input
# Q_crnn_screen_slim400w_more_20w.pb;1:input_0
#Q888_new_detect.tflite;1:input
Q888_model_normalize_object_scene_ps_20200826_f32_no_softmax.tflite;1:input_0 1.5
#Q888_face_emo_dress_mv3_orderd.tflite;1:img
# Q_iMaxDN_RGB_385_p_RGB_RGB_pb2tflite.tflite;1:images cost too much time
Q_iMaxSR_RGB_385_p_pb2tflite.tflite;1:images 1
#bloom_new_detect.tflite;1:input
bloom_model_age_gender.tflite;1:input
hiai_object_detect_814.tflite;1:normalized_input_image_tensor 2
hiai_object_tflite_graph_8bit.tflite;1:normalized_input_image_tensor
# lma_tsec_shallow_channels16_ds2.1.1_model-best-f1.tflite;1:inputs
# lite-model_arbitrary-image-stylization-inceptionv3_fp16_transfer_1.tflite;2:content_image,Conv/BiasAdd
# magenta_arbitrary-image-stylization-v1-256_fp16_transfer_1.tflite;2:content_image,mobilenet_conv/Conv/BiasAdd
# albert_lite_base_squadv1_1.tflite;3:input_ids,input_mask,segment_ids
# mobilebert_1_default_1.tflite;3:input_ids,input_mask,segment_ids
# hdc_tb_cn_neg.tflite;3:input_ids,input_mask,segment_ids 0.5
hiai_cv_labelDetectorModel_v3.tflite;2:input_0,input_1
#ml_tacotron_decoder_step_stf.tflite;9:previous_output,attention,h_0,c_0,h_1,c_1,kappa,encoder_outputs,time;1,80:1,256:1,1024:1,1024:1,1024:1,1024:1,8:1,1,256:1
#ml_headpose_pb2tflite.tflite;3:input_1,batch_normalization_8/batchnorm/add,batch_normalization_1/batchnorm/add;1,64,64,3:16:16
#ml_ei_headpose_pb2tflite.tflite;3:input_1,batch_normalization_8/batchnorm_1/add,batch_normalization_1/batchnorm_1/add;1,64,64,3:16:16
# lite-model_albert_lite_base_squadv1_metadata_1.tflite;3:input_ids,input_mask,segment_ids
# lite-model_mobilebert_1_metadata_1.tflite;3:input_ids,input_mask,segment_ids
# hiai_vad.tflite;2:input,input_cache
add_uint8.tflite;2:input0,input1
coco_ssd_mobilenet_v1_1.0.tflite
# hiai_asr_last_e1_cpu_fast_wavenet_batch1_frame1_one_cache_fp32.tflite;2
# hiai_asr_ctc.tflite;2  buildGraph failed in HarmonyOS(rom 336) and EMUI(rom 330, 511(P50))
# mnasnet_1.0_224.pb;1:input
# mnasnet_1.3_224.pb;1:input
# ml_ocr_jk.pb;1:input_0
# ml_vision_guide_detection1.pb;1:input/input_data
# ml_vision_guide_detection3.pb;1:input/input_data
# hiai_cpu_face_emotion.pb;1:input_0
# hiai_cpu_face_gazing.pb;1:input_0
hiai_cpu_face_headpose.pb;1:input_0 4.5
hiai_ctpn_feature_map.pb;1:input_image 2
# hiai_cv_focusShootOCRModel_02.pb;1:input_0
hiai_cv_focusShootOCRModel_08.pb;1:input 4
hiai_cv_poseEstimation.pb;1:Image 31
hiai_detectmodel_06_23_960_480_1180700.pb;1:input 2.5
# hiai_face_model_npu.pb;1:input_0
# hiai_ghostnet.pb;1:input
hiai_iMaxDN_RGB.pb;1:input
hiai_iMaxSR_RGB.pb;1:input 1
# hiai_latin_ocr.pb;1:input_0
# hiai_latin_ocr_1.pb;1:input_0
hiai_lm_inference_graph.pb;1:image_tensor
hiai_PoseEstimation_Pcm.pb;1:image
# mtk_age_gender.pb;1:img
mtk_model_ckpt.pb;1:input 9.5
# ml_ocr_latin.pb;1:input_0
matmul.pb;1:input0
hiai_ssd_mobilenetv2_object.pb;1:image_tensor 620
hiai_humanDetection.pb;1:normalized_input_image_tensor 165
mtk_face_features_v1.pb;1:input 8.5
# Q_crnn_ori_75w_slim_norm.pb;1:input_0
# Q_crnn_ori_v2_405001_notrans_nopre.pb;1:input_0
# bolt_segment.pb;1:input
# The female/male models and ml_tts_vocoder.pb contain a tensor which is both an input and an output. The tensor name would be keep the same as the output.
# female_model_step2_int16_noiseout.pb;66:cur_mel,noise_next,big_mel_c,upsample_net_conv_in_stack,upsample_net_layers_1_stack,upsample_net_layers_2_stack,upsample_net_layers_3_stack,conv_layers_0_stack,conv_layers_1_stack,conv_layers_2_stack,conv_layers_3_stack,conv_layers_4_stack,conv_layers_5_stack,conv_layers_6_stack,conv_layers_7_stack,conv_layers_8_stack,conv_layers_9_stack,conv_layers_10_stack,conv_layers_11_stack,conv_layers_12_stack,conv_layers_13_stack,conv_layers_14_stack,conv_layers_15_stack,conv_layers_16_stack,conv_layers_17_stack,conv_layers_18_stack,conv_layers_19_stack,conv_layers_20_stack,conv_layers_21_stack,conv_layers_22_stack,conv_layers_23_stack,conv_layers_24_stack,conv_layers_25_stack,conv_layers_26_stack,conv_layers_27_stack,conv_layers_28_stack,conv_layers_29_stack,h_0_stack,h_1_stack,h_2_stack,h_3_stack,h_4_stack,h_5_stack,h_6_stack,h_7_stack,h_8_stack,h_9_stack,h_10_stack,h_11_stack,h_12_stack,h_13_stack,h_14_stack,h_15_stack,h_16_stack,h_17_stack,h_18_stack,h_19_stack,h_20_stack,h_21_stack,h_22_stack,h_23_stack,h_24_stack,h_25_stack,h_26_stack,h_27_stack,h_28_stack
# ml_female_model_step6_noiseout.pb;66:cur_mel,noise_next,big_mel_c,upsample_net_conv_in_stack,upsample_net_layers_1_stack,upsample_net_layers_2_stack,upsample_net_layers_3_stack,conv_layers_0_stack,conv_layers_1_stack,conv_layers_2_stack,conv_layers_3_stack,conv_layers_4_stack,conv_layers_5_stack,conv_layers_6_stack,conv_layers_7_stack,conv_layers_8_stack,conv_layers_9_stack,conv_layers_10_stack,conv_layers_11_stack,conv_layers_12_stack,conv_layers_13_stack,conv_layers_14_stack,conv_layers_15_stack,conv_layers_16_stack,conv_layers_17_stack,conv_layers_18_stack,conv_layers_19_stack,conv_layers_20_stack,conv_layers_21_stack,conv_layers_22_stack,conv_layers_23_stack,conv_layers_24_stack,conv_layers_25_stack,conv_layers_26_stack,conv_layers_27_stack,conv_layers_28_stack,conv_layers_29_stack,h_0_stack,h_1_stack,h_2_stack,h_3_stack,h_4_stack,h_5_stack,h_6_stack,h_7_stack,h_8_stack,h_9_stack,h_10_stack,h_11_stack,h_12_stack,h_13_stack,h_14_stack,h_15_stack,h_16_stack,h_17_stack,h_18_stack,h_19_stack,h_20_stack,h_21_stack,h_22_stack,h_23_stack,h_24_stack,h_25_stack,h_26_stack,h_27_stack,h_28_stack
# ml_male_model_step6_noiseout.pb;66:cur_mel,noise_next,big_mel_c,upsample_net_conv_in_stack,upsample_net_layers_1_stack,upsample_net_layers_2_stack,upsample_net_layers_3_stack,conv_layers_0_stack,conv_layers_1_stack,conv_layers_2_stack,conv_layers_3_stack,conv_layers_4_stack,conv_layers_5_stack,conv_layers_6_stack,conv_layers_7_stack,conv_layers_8_stack,conv_layers_9_stack,conv_layers_10_stack,conv_layers_11_stack,conv_layers_12_stack,conv_layers_13_stack,conv_layers_14_stack,conv_layers_15_stack,conv_layers_16_stack,conv_layers_17_stack,conv_layers_18_stack,conv_layers_19_stack,conv_layers_20_stack,conv_layers_21_stack,conv_layers_22_stack,conv_layers_23_stack,conv_layers_24_stack,conv_layers_25_stack,conv_layers_26_stack,conv_layers_27_stack,conv_layers_28_stack,conv_layers_29_stack,h_0_stack,h_1_stack,h_2_stack,h_3_stack,h_4_stack,h_5_stack,h_6_stack,h_7_stack,h_8_stack,h_9_stack,h_10_stack,h_11_stack,h_12_stack,h_13_stack,h_14_stack,h_15_stack,h_16_stack,h_17_stack,h_18_stack,h_19_stack,h_20_stack,h_21_stack,h_22_stack,h_23_stack,h_24_stack,h_25_stack,h_26_stack,h_27_stack,h_28_stack
# ml_tts_decoder_control_flow.pb;5:h_1,c_1,h_0,decoder_inputs_array,c_0  buildGraph failed in HarmonyOS(rom 336) and EMUI(rom 330, 511(P50))
# ml_tts_vocoder.pb;66:cur_mel,noise_next,big_mel_c,upsample_net_conv_in_stack,upsample_net_layers_1_stack,upsample_net_layers_2_stack,upsample_net_layers_3_stack,conv_layers_0_stack,conv_layers_1_stack,conv_layers_2_stack,conv_layers_3_stack,conv_layers_4_stack,conv_layers_5_stack,conv_layers_6_stack,conv_layers_7_stack,conv_layers_8_stack,conv_layers_9_stack,conv_layers_10_stack,conv_layers_11_stack,conv_layers_12_stack,conv_layers_13_stack,conv_layers_14_stack,conv_layers_15_stack,conv_layers_16_stack,conv_layers_17_stack,conv_layers_18_stack,conv_layers_19_stack,conv_layers_20_stack,conv_layers_21_stack,conv_layers_22_stack,conv_layers_23_stack,conv_layers_24_stack,conv_layers_25_stack,conv_layers_26_stack,conv_layers_27_stack,conv_layers_28_stack,conv_layers_29_stack,h_0_stack,h_1_stack,h_2_stack,h_3_stack,h_4_stack,h_5_stack,h_6_stack,h_7_stack,h_8_stack,h_9_stack,h_10_stack,h_11_stack,h_12_stack,h_13_stack,h_14_stack,h_15_stack,h_16_stack,h_17_stack,h_18_stack,h_19_stack,h_20_stack,h_21_stack,h_22_stack,h_23_stack,h_24_stack,h_25_stack,h_26_stack,h_27_stack,h_28_stack
# hiai_transformer_encoder.pb;15:buffer_in_0,buffer_in_1,buffer_in_2,buffer_in_3,buffer_in_4,buffer_in_5,buffer_in_6,buffer_in_7,buffer_in_8,buffer_in_9,buffer_in_10,buffer_in_11,buffer_in_12,buffer_in_13,encoder_in_deploy
fsr_270_mindspore.pb 0.8
fsr_360_mindspore.pb 0.8
fsr_720_mindspore.pb
# hiai_asr_last_e1_cpu_fast_wavenet_batch1_frame1_one_cache.pb;2
# beard;1:data 120 buildGraph failed in HarmonyOS(rom 336)
# emotion;1:data 2300 buildGraph failed in HarmonyOS(rom 336)
# gender_res_large_deploy;1:data 360 buildGraph failed in HarmonyOS(rom 336)
ml_bank_detect_0312_tmp;1:actual_input_1 7.5
ml_face_div_parsing;1:data 1.5
ml_hardware_eyeclose;1:image_left
# ml_ocr_detect_20200305;1:data
Mnet6_0312_extract_pay;1:data 2
pose_3d;1:data
hiai_face_RFB-Epoch-170-no-transpose;1:input 2
# mtk_face_recognition_v1;1:data
mtk_2012_ATLANTA_10class_20190614_v41;1:data 1
mtk_detect-deeper-halfdeeper-mbv1-lastearlySSD-shortcut-400-400_nopostprocess_simplified;1:input 2.5
detect-deeper-halfdeeper-mbv1-shortcut-400-400_nopostprocess_simplified;1:input 1
hiai_face_detect_rfb;1:input 2
hiai_face_landmark;1:img
# hiai_face_pose_tuku;1:data 110  buildGraph failed in HarmonyOS(rom 336)
ml_hand_detection;1:data 1.5
ml_ocr_cn;1:data 2
ml_ocr_sfz_detect_0325_tmp;1:actual_input_1 1
# ml_hardware_liveness;1:0 500  buildGraph failed in HarmonyOS(rom 336)
ml_liveness_detect_landmark_tmp;1:data
ml_face_contour;1:blob1
2012_ATLANTA_1class_20190621_v4.x_nomean;1:data
ml_ocr_sfz_add_final_0325;1:input
# ml_hardware_pose;1:data 110  buildGraph failed in HarmonyOS(rom 336)
ml_bank_recog;1:input
2012_ATLANTA_10class_20190131_v4.0;1:data 9
# mnet;1:data
# recognition;1:data
ml_face_landmark;1:blob1
model_hebing_3branch;1:input.1 4
hiai_cv_focusShootOCRModel_07;1:data 1.5
hiai_cv_focusShootOCRModel_03;1:data 1
hiai_cv_focusShootOCRModel_01;1:data 4.5
hiai_cv_focusShootOCRModel_04;1:data 3
hiai_cv_focusShootOCRModel_06;1:data 4.5
hiai_cpu_face_hat;1:data
hiai_video_seg;1:data_norm
hiai_semantic_seg;1:data 1.5
hiai_human_seg;1:data 2.5
# hiai_face_recognition_1;1:data
hiai_cpu_face_detect;1:input 2.7
# hiai_cpu_face_attr;1:data 330  buildGraph failed in HarmonyOS(rom 336)
mtk_detect-mbv1-shortcut-400-400_nopostprocess_simplified;1:input 4.5
mtk_detect_mbv1_640_480_nopostprocess_simplified;1:input 2
# retinaface;1:blob1
# deconv_test_model's precision deteriorates in P50 (rom: 511)
deconv_test_model;1:input 12
deconvs_model;1:input
HWSR-s_256_256;1:data 1
age_new;1:data 3
detection_retinaface_fix;1:data 6
landmark;1:img
plat_isface;1:data
PoseNet_dla_17_x512_tmp;1:input 0.6
# ml_location_scene_division's precision deteriorates in P50 (rom: 511)
ml_location_scene_division;1:data 3
ml_tabel_recog;1:input
ml_text_division;1:data 1
ml_handpose;1:blob1 105
ml_lable_model_hebing_device;1:input.1 22
# ml_face_sex;1:data 51  buildGraph failed in HarmonyOS(rom 336)
ml_face_hat;1:data
ml_face_compare;1:blob1 2.5
ml_face_tracking;1:data 1.5
# ml_face_beard;1:data 120  buildGraph failed in HarmonyOS(rom 336)
# ml_face_age;1:data 550  buildGraph failed in HarmonyOS(rom 336)
ml_face_pose;1:data
ml_face_isface;1:data
# ml_face_glasses;1:data 68  buildGraph failed in HarmonyOS(rom 336)
ml_segmentation_matting;1:data 4
ml_segmentation_atlanta_10;1:data 4
ml_bodymask;1:image 7.5
ml_Hand_deploy;1:data 3
ml_hand_3d_detection;1:data 3.5
ml_hand_3d_regression;1:depth 1
ml_ARengine23_bodypose;1:image 64
ml_ocr_bank_card_detection_inception_tmp;1:actual_input_1 6.5
ml_ocr_bank_card_recognition_fcny;1:input
# hiai_cv_aestheticsEngineModel_osp;1:data 3100  buildGraph failed in HarmonyOS(rom 336)
bank_card_recognition_fcny;1:0 1.5
bank_card_detection_inception_tmp;1:actual_input_1 6
ml_ocr_identify_card_fcny;1:input
ml_ocr_identify_card_detect_tmp;1:actual_input_1 1
identify_card_detect_tmp;1:actual_input_1
ml_2012_ocr_rec_caffe;1:input
ml_2012_ocr_detection_caffe_tmp;1:actual_input_1
# ml_face_mnet;1:blob1
ml_segmentation_atlanta_1;1:data
bolt_deploy_color-server;1:data_norm
ml_face_emotion;1:blob1
hdc_ocr_recog_horizontal;1:input
ml_Heatmap_depth_240180;2:depth,heatmaps_point 5
ml_Heatmap_depth_180240;2:depth,heatmaps_point 1.5
mtk_detect-mbv2-shortcut-400-400-simplified.onnx 4
# mtk_face_features_v3.onnx
emotion-ferplus-8.onnx
# rcnn-ilsvrc13-9.onnx
efficientnet-lite4-11.onnx 1
mobilenetv2-7.onnx 3.5
# shufflenet-v2-10.onnx
squeezenet1.1-7.onnx
densenet-9.onnx 100
ml_table_detection_fp32_tmp.onnx
ml_table_segment.onnx
googlenet-9.onnx
inception-v1-9.onnx
#inception-v2-9.onnx 855  #error is too big
# shufflenet-9.onnx
squeezenet1.0-9.onnx 3
residual_distill_cifar10_bs_1.onnx
residual_distill_cifar10_bs_32.onnx
residual_distill_bs_1.onnx 2.5
residual_distill_bs_32.onnx 12
ml_face_3d.onnx 1
gts_version-RFB-320_simplified.onnx 2
mnist-8.onnx 1
# ml_2012_ocr_cn.onnx buldGraph failed when shape fusion is enabled
# ml_2012_ocr_cn_noLSTM.onnx
# candy-9.onnx 1
# mosaic-9.onnx 1
# pointilism-9.onnx
# rain-princess-9.onnx 1
# udnie-9.onnx 1.5
# adversarial_pruning.onnx
residual_distill_res34_cifar10_bs_1_update.onnx 2.1
residual_distill_res50_cifar10_bs_1_update.onnx 1.3
#ml_voice_detect.onnx
# ml_facedetector.onnx 4.5 buildGraph failed when shape fusion is enabled.(hiai BatchMatmul)
ml_ei_facedetection.onnx 1.5
# ml_location_lane_counter.onnx
# ml_location_lane_counter0.onnx
# mtk_emotions-d2012-75.onnx
# mtk_detect-mbv1-shortcut-400-400.onnx
# mtk_detect-mbv2-shortcut-400-400.onnx
# mtk_detect_mbv1_640_480.onnx
mtk_detect-deeper-halfdeeper-mbv1-shortcut-400-400_nopostprocess_simplified_onnx.onnx 2
mtk_detect-mbv1-shortcut-400-400_nopostprocess_simplified_onnx.onnx 3
mtk_detect-deeper-halfdeeper-mbv1-lastearlySSD-shortcut-400-400_nopostprocess_simplified_onnx.onnx 3
# mtk_face_recognition_v3.onnx
# mtk_face_recognition_v2.onnx
ml_2012_ocr_detection_tmp.onnx
bloom_hongmo_detection_tmp.onnx
# Q_face_recognition.onnx
# Q888_face_recognition.onnx
# Q888_iris_detect.onnx
simple_IPS_model_4D_input.onnx
# ml_asr_encoder_int8_202103.onnx 1 buildGraph failed when shape fusion is enabled.(hiai BatchMatmul)
rpnt_pdr_conv2d_16_fixed_last.onnx
hdc_efficientnet_b3_1w_class.onnx 8
# yolov5s.onnx
porseg_tmp.onnx;2 1
# hiai_nlu_onnx_model_v1_0.onnx
# hiai_nlu_onnx_model_v1_1.onnx
# cur acc for ml_audio_kit_vocals_test is 1.7% because the softmax's output of the last op has very small numbers.
# gender_lstm_scd.onnx buildGraph failed when shape fusion is enabled.(hiai BatchMatmul)
# gender_lstm_vad.onnx buildGraph failed when shape fusion is enabled.(hiai BatchMatmul)
#gender_resnet34_lzl.onnx
# ssd-10.onnx
# Q888_CV_face_recognition_self.onnx
# ml_video_edit_dimming_tech_model_styleGan.onnx;2:0,lightFeature
# deeplabv3.r1.1.mindir 1.5
mobilenetv2.r1.1.mindir 0.5
# ssd.r1.1.mindir 0.5 cost too mush time
# deeplabv3_fzy.mindir 1.5
mobilenetv2_fzy.mindir 0.5
video_infer2.tflite;;;;aware_training
ml_video_edit_dimming_tech_model_345000_color.onnx;2
Ireland_face_detector.onnx 1
Ireland_gaze_estimator_ng.onnx 1.5
ml_video_edit_shot_selection_opticalFlow.pb 8.5
ml_video_edit_hair_dyeing_segmodel_20211119 0.5
hiai_asr_ctc.pb;2 4
ml_video_edit_shot_selection_yolox_nano_coco_reduced.onnx 2
ml_video_edit_shot_selection_face_emotion.onnx 0.6
ml_video_edit_face_edit_face3d.onnx 1
ml_video_edit_face_edit_retinaface.onnx;1:input;1,3,120,128;;offline_resize 1.5
ml_video_edit_dimming_tech_model_studio_20.onnx;2 2
# To open, div+conv structure has bug in npu(9000)
# Huawei_video_rvm_mobilenetv3_192.onnx;6 4
# To open, reduce+matmul structure has bug in npu
# ml_video_edit_moon_mode_sky_refine.onnx;2:input0,input1;1,4,256,256:1,4,88,88;;offline_resize 0.5
ml_video_edit_face_edit_pix2pixHD_unet.onnx 2.5
# one of the styleCode_part1.onnx's outputs has values close to 0, which fp16 can not represent well
ml_video_edit_styleCode_part1.onnx 20
ml_video_edit_styleCode_part2.onnx;9 0.5
ml_video_edit_moon_mode_moon_seg 0.5
ml_video_edit_moon_mode_MTI_9c_segmentation_v12 4
ml_video_edit_dynamic_effect_MTI_seg5c_v1 2
# pose_iter's outputs have values close to 0, which fp16 can not represent well
Sport_Health_Tech_pose_iter 12
# the output node argmax is sensitive with relative size of the input value
ml_3D_modeling_AR_wings_densepose_nas_multi_shaobin_v2_addUVhead_mask1_noCls;1:data 25
ml_video_edit_face_cutout_portraitSeg_1.onnx;2
ml_video_edit_face_edit_landmark_855_pose.onnx
# To open, has bug in P40's npu
# ml_video_edit_dynamic_effect_motion_generator_3_50000.onnx;3:input1,input2,input3 5
ml_video_edit_kp_detector_no_dict.onnx;1:image 0.6
ml_ARVR_kitti_192x384
ml_ARVR_MTI_cyberverse1c_v18
ml_ARVR_image2panoindoor.pb;1:input_1;1,256,256,3;;offline_resize 1.8
ml_ARVR_image2hdrlighting.pb 1.1
ml_ARVR_image2weather.pb;1:input;1,224,224,3;;offline_resize 1.5
