mobilenet_v1_0.25_128.tflite 2.5
mobilenet_v2_1.0_224.tflite 2.5
squeezenet.tflite 2.5
inception_resnet_v2.tflite 2
inception_v3.tflite 1
inception_v4.tflite 0.5
efficientnet_lite0_fp32_2.tflite 1
deeplabv3_1_default_1.tflite 2.5
6c_seg_nomean_20200610 1.5
ml_video_edit_person_divison 0.5
ml_video_edit_style_transfer_autoportrait.onnx 9
ml_video_edit_style_transfer_candy.onnx 11
ml_video_edit_style_transfer_gongnongbing.onnx 11
ml_video_edit_style_transfer_starry.onnx 11
porseg_tmp.onnx;2 1
ml_video_edit_Mnet 1.5
ml_video_edit_hairSeg_have_imageProcessLayer_interpTo145 0.5
ml_video_edit_img_segment 1
ml_video_edit_video_segment_gauss_adaptis_part1 2
ml_video_edit_generate_filter.pb 1
ml_video_edit_img_segment_adaptise.pb;2 0.5
ml_video_edit_video_segment_gauss_adaptis_part2.pb;2 10
ml_video_edit_person_divison_pic 0.5
ml_video_edit_person_divison_video;2 13
ml_video_edit_judge.onnx 5
ml_video_edit_vignet.onnx 0.5
hdc_Face_Aesthetic_MTI_Aesthetic 0.5
hdc_Face_Emotion_MTI_Aesthetic.onnx 33
hdc_Face_Landmark5_MTI_Aesthetic.onnx 0.5
hdc_Image_Aesthetic_MTI_Aesthetic.onnx 0.5
hdc_mobilenet_1w_class.onnx 10
hdc_resnet_1w_class.onnx 5
#hdc_age_medium 6
hdc_contour_pose_128 4
ml_face_emotion 0.5
hdc_fivembnet 0.5
ml_face_isface 0.5
hdc_mobilenetface 4
#hdc_retinaface #too many subgraphs
hdc_resnet 3
ml_video_edit_detect_20211111 1
ml_video_edit_hairSeg_have_imageProcessLayer_interpTo145_20210121 0.5
ml_video_edit_have_imageProcessLayer_interpTo145_20201015 0.5
ml_video_edit_MnetN367_extract_1010_pay 0.5
ml_video_edit_reid 0.5
ml_video_edit_v10_best_model_nomean_20200723 8
# hdc_ocr_attention.onnx 0.5 #too many subgraphs
# hdc_ocr_detect.onnx 30 #too many subgraphs
ml_edu_kit_hand_detection.onnx 1
ml_edu_kit_hand_key_position.onnx 2
ml_video_edit_oneclick_adaptis.pb;3 2.4
densenet.tflite 3
resnet_v2_101_299.tflite 1
ml_video_edit_enhance.pb 2
ml_video_edit_video_segment_gauss_adaptis_part2_pb2tflite.tflite;2 10
ml_video_edit_img_segment_adaptise_pb2tflite.tflite;2 0.5
#the fifth value of the ml_video_edit_imitate_filter.onnx's output is very small (10-5).
ml_video_edit_imitate_filter.onnx 200
hdc_mobilenet_1w_class.onnx 20
posenet_mobilenet_float_075_1_default_1.tflite 395
nasnet_mobile.tflite 1
#ml_video_edit_art_generate.onnx, output is out of range
ml_video_edit_art_transfer.onnx;3 3
ml_video_edit_enhance_update_tmp.onnx 0.5
#ml_video_edit_art_generate_20210513.onnx, output is out of range
# ConstructSubgraph change, adjust threshold(3->29) for nlu temporary
# ml_video_edit_art_transfer_20210513.onnx;3 29  memory exceed the npu limitation when shape fusion is enabled
ml_video_edit_hair_dyeing_segmodel_v3 0.5
ml_video_edit_makeup_mobilenetv203.onnx 2
hdc_efficientnet_b3_1w_class.onnx 10
# To open, reduce+matmul structure has bug in npu
# ml_video_edit_moon_mode_sky_refine.onnx;2:input0,input1;1,4,256,256:1,4,88,88;;offline_resize 1
ml_video_edit_styleCode_part2.onnx;9 1.5
# the output node argmax is sensitive with relative size of the input value
ml_3D_modeling_AR_wings_densepose_nas_multi_shaobin_v2_addUVhead_mask1_noCls;1:data 25
ml_video_edit_kp_detector_no_dict.onnx;1:image 0.62
