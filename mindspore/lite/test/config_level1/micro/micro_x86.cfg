[common_quant_param]
# Supports WEIGHT_QUANT or FULL_QUANT
#quant_type=WEIGHT_QUANT
# Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization
# Full quantization support the number of bits [1,8]
#bit_num=8
# Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.
#min_quant_weight_size=0
# Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.
#min_quant_weight_channel=16


[micro_param]
# enable code-generation for MCU HW 
enable_micro=true

# specify HW target, support x86,Cortex-M, ARM32, ARM64 only.
target=x86

# code generation for Inference or Train
codegen_mode=Inference

# enable parallel inference or not
support_parallel=false

# enable debug
debug_mode=false
