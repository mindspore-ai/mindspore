/**
 * Copyright 2023 Huawei Technologies Co., Ltd
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include "backend/common/graph_kernel/bprop_graph_optimizer.h"

#include <memory>
#include <map>
#include <string>
#include <vector>
#include "ops/array_ops.h"
#include "ops/sequence_ops.h"
#include "ops/math_ops.h"
#include "ops/framework_ops.h"
#include "ir/functor.h"
#include "include/common/utils/anfalgo.h"
#include "backend/common/graph_kernel/symbol_engine/symbol_engine.h"
#include "backend/common/graph_kernel/adapter/symbol_engine_builder.h"
#include "backend/common/graph_kernel/symbol_engine/utils.h"
#include "backend/common/graph_kernel/core/graph_kernel_callback.h"

namespace mindspore::graphkernel {
using symbol::IListSymbol;
using symbol::IntSymbol;
using symbol::ListSymbol;
using symbol::ListSymbolPtr;
using symbol::Symbol;

/**
 * Eliminate the ShapeCalc-Reduce-Reshape pattern generated by BroadcastGradientArgs.
 *
 * %0 = Add(a, b)  // when shape of "a" is equal to shape of "%0"
 * ...
 * %10 = ShapeCalc(a, b)
 * %11 = TupleGetItem(%10, 0)
 * %12 = ReduceSum(dout, %11)
 * %13 = Shape(a)
 * %14 = TupleToTensor(%13)
 * %15 = Reshape(%12, %14)
 * %16 = op(%15)
 * --->
 * %0 = Add(a, b)
 * ...
 * %10 = op(dout)
 *
 * Note: there may be another `TupleGetItem(%10, 1)` branch. when both branches are eliminated together, the
 *       `ShapeCalc` is eliminated.
 */
class ElimShapecalcReduceReshape : public opt::PatternProcessPass {
 public:
  ElimShapecalcReduceReshape()
      : PatternProcessPass("elim_shapecalc_reduce_reshape", true),
        dout_{std::make_shared<Var>()},
        shape_calc_{std::make_shared<Var>()},
        index_{std::make_shared<Var>()} {};
  ~ElimShapecalcReduceReshape() override = default;

  bool Run(const FuncGraphPtr &func_graph) override {
    auto nodes = TopoSort(func_graph->output());
    for (auto &node : nodes) {
      auto cnode = node->cast<CNodePtr>();
      if (cnode != nullptr) {
        auto uniq_id = cnode->GetPrimalAttr(kPrimalAttrUniqueId);
        if (uniq_id != nullptr) {
          unique_node_[GetValue<std::string>(uniq_id)] = cnode;
        }
      }
    }
    return NodePass::Run(func_graph);
  }

  const BaseRef DefinePattern() const override {
    VarPtr s = std::make_shared<Var>();
    auto getitem_node = VectorRef({prim::kPrimTupleGetItem, shape_calc_, index_});
    auto reducesum_node = VectorRef({prim::kPrimReduceSum, dout_, getitem_node});
    auto reshape_node = VectorRef({prim::kPrimReshape, reducesum_node, s});
    return reshape_node;
  }

  const AnfNodePtr Process(const FuncGraphPtr &func_graph, const AnfNodePtr &node,
                           const EquivPtr &equiv) const override {
    auto symbol_engine_attr = func_graph->get_attr(kAttrSymbolEngine);
    MS_EXCEPTION_IF_NULL(symbol_engine_attr);
    auto symbol_engine = symbol_engine_attr->cast<SymbolEnginePtr>();
    MS_EXCEPTION_IF_NULL(symbol_engine);
    auto cnode = node->cast<CNodePtr>();
    MS_EXCEPTION_IF_NULL(cnode);
    auto shapecalc_node = utils::cast<CNodePtr>((*equiv)[shape_calc_]);
    constexpr const size_t shapecalc_size = 3;
    if (shapecalc_node == nullptr || !IsPrimitiveCNode(shapecalc_node, prim::kPrimShapeCalc) ||
        shapecalc_node->size() != shapecalc_size) {
      return nullptr;
    }
    auto functor = common::AnfAlgo::GetNodeAttr<ShapeCalcBaseFunctorPtr>(shapecalc_node, kAttrFunctor);
    MS_EXCEPTION_IF_NULL(functor);
    if (functor->name() != "ShapeCalc_BroadcastGradientArgs") {
      // only support the broadcast gradient condition
      return nullptr;
    }
    auto unique_id_iter = cnode->primal_attrs().find(kPrimalAttrForwardUniqueId);
    if (unique_id_iter == cnode->primal_attrs().end()) {
      // only support bprop node
      return nullptr;
    }
    auto unique_node_iter = unique_node_.find(GetValue<std::string>(unique_id_iter->second));
    if (unique_node_iter == unique_node_.end()) {
      return nullptr;
    }
    auto forward_node = unique_node_iter->second;
    MS_LOG(DEBUG) << "The fowrard node name of " << cnode->fullname_with_scope() << " is "
                  << AnfUtils::GetCNodeName(forward_node);

    auto index_value_node = utils::cast<ValueNodePtr>((*equiv)[index_]);
    MS_EXCEPTION_IF_NULL(index_value_node);
    auto idx = GetValue<int64_t>(index_value_node->value());  // idx is 0 or 1
    auto grad = symbol_engine->QuerySymbolicShape(forward_node->input(idx + 1));
    auto out = symbol_engine->QuerySymbolicShape(forward_node);
    MS_EXCEPTION_IF_NULL(out);
    if (!CheckElimReduce(grad, out, GetValue<size_t>(functor->ToValue()))) {
      return nullptr;
    }
    MS_LOG(INFO) << "Skip the " << cnode->fullname_with_scope()
                 << " generated by BroadcastGradientArgs. grad symbol: " << grad->ToString()
                 << ". out symbol: " << (out != nullptr ? out->ToString() : "null");
    return utils::cast<AnfNodePtr>((*equiv)[dout_]);
  }

  bool CheckElimReduce(const ListSymbolPtr &grad_input, const ListSymbolPtr &output, size_t shift) const {
    if (grad_input == nullptr || !grad_input->is<IListSymbol>()) {
      return false;
    }
    if (grad_input->size() < output->size()) {
      return false;
    }
    for (size_t i = grad_input->size(); i > shift; i--) {
      auto gs = grad_input->symbols()[grad_input->size() - i]->as<IntSymbol>();
      MS_EXCEPTION_IF_NULL(gs);
      if (gs->is_greater_than(1)) {
        continue;
      }
      if (i <= output->size() && !gs->EqualsTo(output->symbols()[output->size() - i])) {
        return false;
      }
    }
    return true;
  }

 private:
  VarPtr dout_;
  VarPtr shape_calc_;
  VarPtr index_;
  std::map<std::string, CNodePtr> unique_node_;
};

// when input shape and output shape are equal, the reshape can be eliminated.
class ElimReshape : public opt::PatternProcessPass {
 public:
  ElimReshape() : PatternProcessPass("elim_reshape", true), input_{std::make_shared<Var>()} {};
  ~ElimReshape() override = default;

  const BaseRef DefinePattern() const override {
    auto shape = std::make_shared<Var>();
    auto reshape_node = VectorRef({prim::kPrimReshape, input_, shape});
    return reshape_node;
  }

  const AnfNodePtr Process(const FuncGraphPtr &func_graph, const AnfNodePtr &node,
                           const EquivPtr &equiv) const override {
    auto symbol_engine_attr = func_graph->get_attr(kAttrSymbolEngine);
    MS_EXCEPTION_IF_NULL(symbol_engine_attr);
    auto symbol_engine = symbol_engine_attr->cast<SymbolEnginePtr>();
    MS_EXCEPTION_IF_NULL(symbol_engine);
    auto input_node = utils::cast<AnfNodePtr>((*equiv)[input_]);
    auto input_shape = symbol_engine->QuerySymbolicShape(input_node);
    auto output_shape = symbol_engine->QuerySymbolicShape(node);
    if (input_shape != nullptr && input_shape->EqualsTo(output_shape)) {
      MS_LOG(INFO) << "For node " << node->DebugString() << " (" << node->fullname_with_scope()
                   << "), the input shape and output shape is same, which can be eliminated.";
      return input_node;
    }
    return nullptr;
  }

 private:
  VarPtr input_;
};

// the symbolic value of "shape" is static or has only one "-1", replace the "shape" to a const tensor.
class ReshapeOptShape : public opt::PatternProcessPass {
 public:
  ReshapeOptShape() : PatternProcessPass("reshape_opt_shape", true), shape_{std::make_shared<Var>()} {};
  ~ReshapeOptShape() override = default;

  const BaseRef DefinePattern() const override {
    auto inp = std::make_shared<Var>();
    auto reshape_node = VectorRef({prim::kPrimReshape, inp, shape_});
    return reshape_node;
  }

  const AnfNodePtr Process(const FuncGraphPtr &func_graph, const AnfNodePtr &node,
                           const EquivPtr &equiv) const override {
    auto symbol_engine_attr = func_graph->get_attr(kAttrSymbolEngine);
    MS_EXCEPTION_IF_NULL(symbol_engine_attr);
    auto symbol_engine = symbol_engine_attr->cast<SymbolEnginePtr>();
    MS_EXCEPTION_IF_NULL(symbol_engine);
    auto shape_node = utils::cast<AnfNodePtr>((*equiv)[shape_]);
    if (shape_node->isa<ValueNode>()) {
      return nullptr;
    }
    auto shape = symbol_engine->QuerySymbolicValue(shape_node);
    if (shape == nullptr) {
      return nullptr;
    }
    auto shape_vec = symbol::ToShape(shape);
    if (std::count(shape_vec.cbegin(), shape_vec.cend(), -1) > 1) {
      return nullptr;
    }
    MS_LOG(INFO) << "For node " << node->DebugString() << " (" << node->fullname_with_scope()
                 << "), the \"shape\" can be replace to a const tensor. shape symbol is: " << shape->ToString();
    auto cnode = node->cast<CNodePtr>();
    MS_EXCEPTION_IF_NULL(cnode);
    auto shape_tensor = NewValueNode(std::make_shared<tensor::Tensor>(shape_vec, kInt64));
    shape_tensor->set_abstract(shape_tensor->value()->ToAbstract());
    Callback::Instance()->SetBasicNodeKernelInfo(
      shape_tensor, {{ShapeVector{SizeToLong(shape_vec.size())}, kNumberTypeInt64, kOpFormat_DEFAULT}});
    auto newnode = func_graph->NewCNode({cnode->input(kIndex0), cnode->input(kIndex1), shape_tensor});
    newnode->set_abstract(node->abstract());
    newnode->set_kernel_info(node->kernel_info_ptr());
    newnode->set_fullname_with_scope(node->fullname_with_scope());
    symbol_engine->BuildCNodeSymbol(newnode, false);
    return newnode;
  }

 private:
  VarPtr shape_;
};

bool BpropGraphOptimizer::Run(const FuncGraphPtr &func_graph) {
  std::vector<opt::PassPtr> passes = {
    std::make_shared<SymbolEngineBuilder>(false),  // build symbol engine for whole graph
    std::make_shared<ElimShapecalcReduceReshape>(),
    std::make_shared<ElimReshape>(),
    std::make_shared<ReshapeOptShape>(),
    std::make_shared<ElimReshape>(),
  };
  bool changed = false;
  for (auto &p : passes) {
    changed = p->Run(func_graph) || changed;
  }
  return changed;
}
}  // namespace mindspore::graphkernel
