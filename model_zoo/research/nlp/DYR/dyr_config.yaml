# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
# Url for modelarts
data_url: ""
train_url: ""
checkpoint_url: ""
# Path for local
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
device_target: "Ascend"
enable_profiling: False
description: "run_dyr"
# DYR version: "dry_base" or "dyr"
dyr_version: "dry_base"
do_train: "true"
do_eval: "true"
device_id: 0
epoch_num: 1
group_size: 8
group_num: 1
train_data_shuffle: "true"
eval_data_shuffle: "false"
train_batch_size: 1
eval_batch_size: 1
save_finetune_checkpoint_path: "./classifier_finetune/ckpt/"
load_pretrain_checkpoint_path: ""
load_finetune_checkpoint_path: ""
train_data_file_path: ""
eval_data_file_path: ""
eval_ids_path: "ids.tsv"
eval_qrels_path: "msmarco-docdev-qrels.tsv"
save_score_path: "score.txt"
schema_file_path: ""

optimizer_cfg:
    optimizer: 'Lamb'
    AdamWeightDecay:
        learning_rate: 0.00001  # 1e-5
        end_learning_rate: 0.0000000001  # 1e-10
        power: 1.0
        weight_decay: 0.01  # 1e-5
        decay_filter: ['layernorm', 'bias']
        eps: 0.000001  # 1e-6
    Lamb:
        learning_rate: 0.00001  # 1e-5,
        end_learning_rate: 0.0000001  # 1e-7
        power: 1.0
        weight_decay: 0.01
        decay_filter: ['layernorm', 'bias']
    Momentum:
        learning_rate: 0.00002  # 2e-5
        momentum: 0.9

dyr_net_cfg:
    seq_length: 512
    vocab_size: 30522
    hidden_size: 768
    num_hidden_layers: 12
    num_attention_heads: 12
    intermediate_size: 3072
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: False
    dtype: mstype.float32
    compute_type: mstype.float16

---
# Help description for each configuration
enable_modelarts: "Whether training on modelarts, default: False"
data_url: "Url for modelarts"
train_url: "Url for modelarts"
dyr_version: "dyr version"
data_path: "The location of the input data."
output_path: "The location of the output file."
device_target: "Running platform, choose from Ascend or GPU, now only supports Ascend."
enable_profiling: 'Whether enable profiling while training, default: False'
do_train: "Enable train, default is false"
do_eval: "Enable eval, default is false"
device_id: "Device id, default is 0."
epoch_num: "Epoch number, default is 3."
group_size: "Sample number in one block."
group_num: "Cards are divided into groups."
train_data_shuffle: "Enable train data shuffle, default is true"
eval_data_shuffle: "Enable eval data shuffle, default is false"
train_batch_size: "Train batch size, default is 32"
eval_batch_size: "Eval batch size, default is 1"
save_finetune_checkpoint_path: "Save checkpoint path"
load_pretrain_checkpoint_path: "Load checkpoint file path"
load_finetune_checkpoint_path: "Load checkpoint file path"
train_data_file_path: "Data path, it is better to use absolute path"
eval_data_file_path: "Data path, it is better to use absolute path"
eval_ids_path: "Ids path, it is better to use absolute path"
eval_qrels_path: "Qrels path, it is better to use absolute path"
save_score_path: "Score path, it is better to use absolute path"
schema_file_path: "Schema path, it is better to use absolute path"
---
device_target: ['Ascend']
do_train: ["true", "false"]
do_eval: ["true", "false"]
train_data_shuffle: ["true", "false"]
eval_data_shuffle: ["true", "false"]