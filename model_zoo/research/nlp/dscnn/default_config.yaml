# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unlesee you know exactly what you are doing)
enable_modelarts: False
# url for modelarts
data_url: ""
train_url: ""
checkpoint_url: ""
# path for local
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
device_target: "Ascend"
enable_profiling: False

# ======================================================================================
# common options
device_id: 0
is_distributed: False
wanted_words: "yes,no,up,down,left,right,on,off,stop,go"
sample_rate: 16000
clip_duration_ms: 1000
window_size_ms: 40.0
window_stride_ms: 20.0
dct_coefficient_count: 20
drop: 0.9
per_batch_size: 100

# network related
model_size_info: [6, 276, 10, 4, 2, 1, 276, 3, 3, 2, 2, 276, 3, 3, 1, 1, 276, 3, 3, 1, 1, 276, 3, 3, 1,
                                 1, 276, 3, 3, 1, 1
]
# ======================================================================================
# Training options
amp_level: "O0"

# dataset options
download_data_url: "http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz"
data_dir: "data"
download_feat_dir: "./dscnn_dataset"
train_feat_dir: ""
background_volume: 0.1
background_frequency: 0.8
silence_percentage: 10.0
unknown_percentage: 10.0
time_shift_ms: 100.0
testing_percentage: 10
validation_percentage: 10
pretrained: ""

# training related
use_graph_mode: 1
val_interval: 1

# optimizer and lr related
lr_scheduler: "multistep"
lr: 0.1
lr_epochs: "20,40,60,80"
lr_gamma: 0.1
eta_min: 0.0
T_max: 80
max_epoch: 80
warmup_epochs: 0.0
weight_decay: 0.001
momentum: 0.98

# logging related
log_interval: 100
save_ckpt_path: "./checkpoint"
ckpt_interval: 100

# ======================================================================================
# Eval options
eval_feat_dir: ""
model_dir: ""
log_path: "./eval_outputs/"

# ======================================================================================
# export options
export_ckpt_path: ""
file_name: "dscnn"
file_format: "MINDIR"

# ======================================================================================
# preprocess options
pre_result_path: "preprocess_Result"
pre_feat_dir: ""

# ======================================================================================
# postprocess options
label_path: ""
post_result_path: ""

---
# Help description for each configuration
enable_modelarts: "Whether training on modelarts default: False"
download_data_url: "Url for modelarts"
train_url: "Url for modelarts"
data_path: "The location of input data"
output_pah: "The location of the output file"
device_target: "device id of GPU or Ascend. (Default: None)"
enable_profiling: "Whether enable profiling while training default: False"
file_name: "CNN&CTC output air name"
file_format: "choices [AIR, MINDIR]"
val_ckpt_path: "checkpoint path."
is_distributed: "distributed training"
device_id: "which device the model will be trained on"
amp_level: "choices=['O3', 'O2', 'O0']"
data_url: "Location of speech training data archive on the web."
data_dir: "Where to download the dataset."
train_feat_dir: "Where to save the feature of audios"
background_volume: "How loud the background noise should be, between 0 and 1."
background_frequency: "How many of the training samples have background noise mixed in."
silence_percentage: "How much of the training data should be silence."
unknown_percentage: "How much of the training data should be unknown words."
time_shift_ms: "Range to randomly shift the training audio by in time."
testing_percentage: "What percentage of wavs to use as a test set."
validation_percentage: "What percentage of wavs to use as a validation set."
wanted_words: "Words to use (others will be added to an unknown label)"
sample_rate: "Expected sample rate of the wavs"
clip_duration_ms: "Expected duration in milliseconds of the wavs"
window_size_ms: "How long each spectrogram timeslice is"
window_stride_ms: "How long each spectrogram timeslice is"
dct_coefficient_count: "How many bins to use for the MFCC fingerprint"
model_size_info: "Model dimensions - different for various models"
drop: "dropout"
pretrained: "model_path, local pretrained model to load"
use_graph_mode: "use graph mode or feed mode"
val_interval: "validate interval"
per_batch_size: "batch size for per gpu"
lr_scheduler: "lr-scheduler, option type: multistep, cosine_annealing"
lr: "learning rate of the training"
lr_epochs: "epoch of lr changing"
lr_gamma: "decrease lr by a factor of exponential lr_scheduler"
eta_min: "eta_min in cosine_annealing scheduler"
T_max: "T-max in cosine_annealing scheduler"
max_epoch: "max epoch num to train the model"
warmup_epochs: "warmup epoch"
weight_decay: "weight decay"
momentum: "momentum"
log_interval: "logging interval"
ckpt_path: "checkpoint save location"
ckpt_interval: "save ckpt_interval"
model_dir: "which folder the models are saved in or specific path of one model"
log_path: "path to save eval log"
