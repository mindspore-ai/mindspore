# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unlesee you know exactly what you are doing)
enable_modelarts: False
# url for modelarts
data_url: ""
train_url: ""
checkpoint_url: ""
# path for local
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
device_target: "Ascend"
enable_profiling: False
need_modelarts_dataset_unzip: True
modelarts_dataset_unzip_name: "MindRecord_COCO"

# ======================================================================================
# common options
distribute: False

# ======================================================================================
# create dataset
create_dataset: "coco"
prefix: "retinanet.mindrecord"
is_training: True

# ======================================================================================
# Training options
img_shape: [600, 600]
num_retinanet_boxes: 67995
match_thershold: 0.5
nms_thershold: 0.6
min_score: 0.1
max_boxes: 100

# learning rate settings
lr: 0.1
global_step: 0
lr_init: 1e-6
lr_end_rate: 5e-3
warmup_epochs1: 2
warmup_epochs2: 5
warmup_epochs3: 23
warmup_epochs4: 60
warmup_epochs5: 160
momentum: 0.9
weight_decay: 1.5e-4

# network
num_default: [9, 9, 9, 9, 9]
extras_out_channels: [256, 256, 256, 256, 256]
feature_size: [75, 38, 19, 10, 5]
aspect_ratios: [[0.5, 1.0, 2.0], [0.5, 1.0, 2.0], [0.5, 1.0, 2.0], [0.5, 1.0, 2.0], [0.5, 1.0, 2.0]]
steps: [8, 16, 32, 64, 128]
anchor_size: [32, 64, 128, 256, 512]
prior_scaling: [0.1, 0.2]
gamma: 2.0
alpha: 0.75
num_classes: 81

# `mindrecord_dir` and `coco_root` are better to use absolute path.
mindrecord_dir: "./"
coco_root: "./"
train_data_type: "train2017"
val_data_type: "val2017"
instances_set: "annotations/instances_{}.json"
coco_classes: ["background", "person", "bicycle", "car", "motorcycle", "airplane", "bus",
                     "train", "truck", "boat", "traffic light", "fire hydrant",
                     "stop sign", "parking meter", "bench", "bird", "cat", "dog",
                     "horse", "sheep", "cow", "elephant", "bear", "zebra",
                     "giraffe", "backpack", "umbrella", "handbag", "tie",
                     "suitcase", "frisbee", "skis", "snowboard", "sports ball",
                     "kite", "baseball bat", "baseball glove", "skateboard",
                     "surfboard", "tennis racket", "bottle", "wine glass", "cup",
                     "fork", "knife", "spoon", "bowl", "banana", "apple",
                     "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza",
                     "donut", "cake", "chair", "couch", "potted plant", "bed",
                     "dining table", "toilet", "tv", "laptop", "mouse", "remote",
                     "keyboard", "cell phone", "microwave", "oven", "toaster", "sink",
                     "refrigerator", "book", "clock", "vase", "scissors",
                     "teddy bear", "hair drier", "toothbrush"]


# The annotation.json position of voc validation dataset
voc_root: ""

# voc original dataset
voc_dir: ""

# if coco or voc used, `image_dir` and `anno_path` are useless
image_dir: ""
anno_path: ""
save_checkpoint: True
save_checkpoint_epochs: 1
keep_checkpoint_max: 10
save_checkpoint_path: "./ckpt"
finish_epoch: 0

# optimiter options
workers: 24
mode: "sink"
epoch_size: 500
batch_size: 32
pre_trained: ""
pre_trained_epoch_size: 0
loss_scale: 1024
filter_weight: False

# ======================================================================================
# Eval options
dataset: "coco"
checkpoint_path: ""

# ======================================================================================
# export options
device_id: 0
file_format: "MINDIR"
export_batch_size: 1
file_name: "retinanet"

# ======================================================================================
# postprocess options
result_path: ""
img_path: ""
img_id_file: ""

---
# Help description for each configuration
enable_modelarts: "Whether training on modelarts default: False"
data_url: "Url for modelarts"
train_url: "Url for modelarts"
data_path: "The location of input data"
output_pah: "The location of the output file"
device_target: "device id of GPU or Ascend. (Default: None)"
enable_profiling: "Whether enable profiling while training default: False"
workers: "Num parallel workers."
lr: "Learning rate, default is 0.1."
mode: "Run sink mode or not, default is sink."
epoch_size: "Epoch size, default is 500."
batch_size: "Batch size, default is 32."
pre_trained: "Pretrained Checkpoint file path."
pre_trained_epoch_size: "Pretrained epoch size."
save_checkpoint_epochs: "Save checkpoint epochs, default is 1."
loss_scale: "Loss scale, default is 1024."
filter_weight: "Filter weight parameters, default is False."
dataset: "Dataset, default is coco."
device_id: "Device id, default is 0."
file_format: "file format choices [AIR, MINDIR]"
file_name: "output file name."
export_batch_size: "batch size"
result_path: "result file path."
img_path: "image file path."
img_id_file: "image id file."
