# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
data_url: ""
train_url: ""
checkpoint_url: ""
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
checkpoint_path: './checkpoint/'
device_target: Ascend
enable_profiling: False

# ==============================================================================
# Config setup
flip_idx: [[0, 1], [3, 4]]
default_resolution: [512, 512]
heads: {'hm': 1, 'wh': 2, 'hm_offset': 2, 'landmarks': 5 * 2}
head_conv: 64
max_objs: 64

rand_crop: True
scale: 0.4
shift: 0.1
aug_rot: 0
color_aug: True
flip: 0.5
input_res: 512 #768 #800
output_res: 128 #192 #200
num_classes: 1
num_joints: 5
reg_offset: True
hm_hp: True
reg_hp_offset: True
dense_hp: False
hm_weight: 1.0
wh_weight: 0.1
off_weight: 1.0
lm_weight: 0.1
rotate: 0

# for test
mean: [0.408, 0.447, 0.470]
std: [0.289, 0.274, 0.278]
test_scales: [0.999,]
nms: 1
flip_test: 0
fix_res: True
input_h: 832 #800
input_w: 832 #800
K: 200
down_ratio: 4
test_batch_size: 1

master_batch_size: 8
num_workers: 8
not_rand_crop: False
no_color_aug: False

# ==============================================================================
# train.py mindspore coco training
# dataset related
data_dir: '/cache/data/'
annot_path: '/cache/data/annotations/train_wider_face.json'
img_dir: '/cache/data/images/WIDER_train/images/'
per_batch_size: 8

# network related
pretrained_backbone: '/cache/data/centerface/mobilenet_v2.ckpt'
resume: ''

# optimizer and lr related
lr_scheduler: 'multistep'
lr: 0.004 # 4e-3
lr_epochs: '90,120'
lr_gamma: 0.1
eta_min: 0.
t_max: 140
max_epoch: 140
warmup_epochs: 0
weight_decay: 0.0005
momentum: 0.9
optimizer: 'adam'

# loss related
loss_scale: 1024
label_smooth: 0
label_smooth_factor: 0.1

 # logging related
log_interval: 100
ckpt_path: './output'
ckpt_interval: 0 # None

is_save_on_master: 1

# distributed related
is_distributed: 1
rank: 0
group_size: 1

# roma obs
#train_url: ""

# profiler init, can open when you debug. if train, donot open, since it cost memory and disk space
need_profiler: 0

# reset default config
training_shape: ""
resize_rate: 0 # None

# test.py mindspore coco training'
test_model: ''
ground_truth_mat: ''
save_dir: ''
ground_truth_path: ''
eval: 0
eval_script_path: ''
ckpt_name: ""
device_num: 1
steps_per_epoch: 198
start: 0
end: 18

# export.py centerface export'
device_id: 0
batch_size: 1
ckpt_file: ''
file_name: "centerface"
file_format: 'AIR'

# centerface preprocess"
dataset_path: ''
preprocess_path: ''

# postprocess / centerface calcul AP
result_path: ''
label_path: ''
meta_path: ''
save_path: ''

# ==============================================================================
# src/convert_weight.py
ckpt_fn: '/model_path/centerface.ckpt'
pt_fn: '/model_path/centerface.pth'
out_fn: '/model_path/centerface_out.ckpt'
pt2ckpt: 1

# src/convert_weight_mobilenetv2.py
ckpt_fn_v2: '/model_path/mobilenet_v2_key.ckpt'
pt_fn_v2: '/model_path/mobilenet_v2-b0353104.pth'
out_ckpt_fn: '/model_path/mobilenet_v2-b0353104.ckpt'


---
# Config description for each option
enable_modelarts: 'Whether training on modelarts, default: False'
data_url: 'Dataset url for obs'
train_url: 'Training output url for obs'
data_path: 'Dataset path for local'
output_path: 'Training output path for local'

data_dir: 'train data dir'
annot_path: 'train data annotation path'
img_dir: 'train data img dir'
per_batch_size: 'batch size for per gpu'
pretrained_backbone: 'model_path, local pretrained backbone model to load'
resume: 'path of pretrained centerface_model'
lr_scheduler: 'lr-scheduler, option type: exponential, cosine_annealing'
lr: 'learning rate of the training'
lr_epochs: 'epoch of lr changing'
lr_gamma: 'decrease lr by a factor of exponential lr_scheduler'
eta_min: 'eta_min in cosine_annealing scheduler'
t_max: 'T-max in cosine_annealing scheduler'
max_epoch: 'max epoch num to train the model'
warmup_epochs: 'warmup epoch'
weight_decay: 'weight decay'
momentum: 'momentum'
optimizer: 'optimizer type, default: adam'
loss_scale: 'static loss scale'
label_smooth: 'whether to use label smooth in CE'
label_smooth_factor: 'smooth strength of original one-hot'
log_interval: 'logging interval'
ckpt_path: 'checkpoint save location'
ckpt_interval: 'ckpt_interval'
is_save_on_master: 'save ckpt on master or all rank'
is_distributed: 'if multi device'
rank: 'local rank of distributed'
group_size: 'world size of distributed'
need_profiler: 'whether use profiler'
training_shape: 'fix training shape'
resize_rate: 'resize rate for multi-scale training'
test_model: 'test model dir'
ground_truth_mat: 'ground_truth, mat type'
save_dir: 'save_path for evaluate'
ground_truth_path: 'ground_truth path, contain all mat file'
eval: 'if do eval after test'
eval_script_path: 'evaluate script path'
ckpt_name: 'input model name'
steps_per_epoch: 'steps for each epoch'
start: 'start loop number, used to calculate first epoch number'
end: 'end loop number, used to calculate last epoch number'
batch_size: "batch size"
ckpt_file: "Checkpoint file path."

ckpt_fn: 'ckpt for user to get cell/module name'
pt_fn: 'checkpoint filename to convert'
out_fn: 'convert output ckpt/pth path'
pt2ckpt: '1 : pt2ckpt; 0 : ckpt2pt'

enable_profiling: 'Whether enable profiling while training, default: False'
only_create_dataset: 'If set it true, only create Mindrecord, default is false.'
run_distribute: 'Run distribute, default is false.'
do_train: 'Do train or not, default is true.'
do_eval: 'Do eval or not, default is false.'
dataset: 'Dataset, default is coco.'
pre_trained: 'Pretrain file path.'
device_id: 'Device id, default is 0.'
device_num: 'Use device nums, default is 1.'
rank_id: 'Rank id, default is 0.'
file_name: "output file name."
file_format: 'file format'
img_path: "image file path."
result_path: "result file path."

dataset_path: "dataset path."
preprocess_path: "preprocess path."
label_file: "label file"
meta_file: "label file"
save_path: "label file"

---
platform: ['Ascend', 'GPU', 'CPU']
file_format: ["AIR", "MINDIR"]
freeze_layer: ["", "none", "backbone"]