# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
data_url: ""
train_url: ""
checkpoint_url: ""
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
device_target: Ascend
enable_profiling: False

# ==============================================================================
# argparse_init 'WideDeep'
epochs: 15
full_batch: False
batch_size: 16000
eval_batch_size: 16000
field_size: 39
vocab_size: 200000
vocab_cache_size: 0
emb_dim: 80
deep_layer_dim: [1024, 512, 256, 128]
deep_layer_act: 'relu'
keep_prob: 1.0
dropout_flag: False
ckpt_path: "./ckpt"
stra_ckpt: "./checkpoints"
eval_file_name: "eval.log"
loss_file_name: "loss.log"
host_device_mix: 0
dataset_type: "mindrecord"
parameter_server: 0
field_slice: False
sparse: False
deep_table_slice_mode: "column_slice"

# WideDeepConfig
#data_path: "./test_raw_data/"
#vocab_cache_size: 100000
#stra_ckpt: './checkpoints/strategy.ckpt'
weight_bias_init: ['normal', 'normal']
emb_init: 'normal'
init_args: [-0.01, 0.01]
l2_coef: 0.00008 # 8e-5
manual_shape: None
#if self.host_device_mix == 1:
#    self.sparse = True

# ==============================================================================
# wide_and_deep export
device_id: 0
ckpt_file: "/cache/train/widedeep_train-15_2582.ckpt"
file_name: "wide_and_deep"
file_format: "AIR"

#'postprocess'
result_path: "./result_Files"
label_path: ''

# 'preprocess.'
dataset_path: ''
#result_path: './preprocess_Result'

# src/process_data.py  "Get and Process datasets"
raw_data_path: "./raw_data"

# src/preprocess_data.py  "Recommendation dataset"
dense_dim: 13
slot_dim: 26
threshold: 100
train_line_count: 45840617
skip_id_convert: 0

# src/generate_synthetic_data.py 'Generate Synthetic Data'
output_file: "./train.txt"
label_dim: 2
number_examples: 4000000
vocabulary_size: 400000000
random_slot_values: 0

---
# Config description for each option
enable_modelarts: 'Whether training on modelarts, default: False'
data_url: 'Dataset url for obs'
train_url: 'Training output url for obs'
data_path: 'Dataset path for local'
output_path: 'Training output path for local'

device_target: "device target, support Ascend, GPU and CPU."
dataset_path: 'Dataset path'
batch_size: "batch size"
ckpt_path: 'Checkpoint path'
eval_file_name: 'Auc log file path. Default: "./auc.log"'
loss_file_name: 'Loss log file path. Default: "./loss.log"'
do_eval: 'Do evaluation or not, only support "True" or "False". Default: "True"'
checkpoint_path: 'Checkpoint file path'
device_id: "Device id"
ckpt_file: "Checkpoint file path."
file_name: "output file name."
file_format: "file format"
result_path: 'Result path'
# result_path: "./result_Files" # 'result path'
label_path: 'label path'
host_device_mix: "Enable host device mode or not"
dataset_type: "tfrecord/mindrecord/hd5"
parameter_server: "Open parameter server of not"
field_slice: "Enable split field mode or not"
sparse: "Enable sparse or not"
deep_table_slice_mode: "column_slice/row_slice"

epochs: "Total train epochs"
full_batch: "Enable loading the full batch"
eval_batch_size: "Eval batch size."
field_size: "The number of features."
vocab_size: "The total features of dataset."
vocab_cache_size: "The total features of hash table."
emb_dim: "The dense embedding dimension of sparse feature."
deep_layer_dim: "The dimension of all deep layers."
deep_layer_act: "The activation function of all deep layers."
keep_prob: "The keep rate in dropout layer."
dropout_flag: "Enable dropout"
stra_ckpt: "The strategy checkpoint file."

dense_dim: 'The number of your continues fields'
slot_dim: 'The number of your sparse fields, it can also be called catelogy features.'
threshold: 'Word frequency below this will be regarded as OOV. It aims to reduce the vocab size'
train_line_count: 'The number of examples in your dataset'
skip_id_convert: 'Skip the id convert, regarding the original id as the final id.'
raw_data_path: "The path to save dataset"
output_file: 'The output path of the generated file'
label_dim: 'The label category'
number_examples: 'The row numbers of the generated file'
vocabulary_size: "The vocabulary size of the total dataset"
random_slot_values: "If 1, the id is geneted by the random. If false, the id is set by the row_index mod part_size, where part_size the the vocab size for each slot"

---
device_target: ['Ascend', 'GPU', 'CPU']
file_format: ["AIR", "ONNX", "MINDIR"]
freeze_layer: ["", "none", "backbone"]
skip_id_convert: [0, 1]
dataset_type: ["tfrecord", "mindrecord", "hd5"]
