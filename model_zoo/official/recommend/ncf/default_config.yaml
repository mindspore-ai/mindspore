# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
# Url for modelarts
data_url: ""
train_url: ""
checkpoint_url: ""
# Path for local
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
device_target: "Ascend"
enable_profiling: False

# ==============================================================================
# Training options
dataset: "ml-1m"
train_epochs: 14
batch_size: 256
eval_batch_size: 160000
num_neg: 4
layers: [64, 32, 16]
num_factors: 16
checkpoint_path: "./checkpoint/"

# Eval options
eval_file_name: "eval.log"
checkpoint_file_path: "./checkpoint/NCF-14_19418.ckpt"

# Export options
device_id: 0
ckpt_file: "./checkpoint/NCF-25_19418.ckpt"
file_name: "ncf"
file_format: "MINDIR"

# Preprocess options
pre_result_path: "./preprocess_Result"

# Postprocess options
post_result_path: "./result_Files"

---

# Help description for each configuration
enable_modelarts: "Whether training on modelarts, default: False"
data_url: "Url for modelarts"
train_url: "Url for modelarts"
data_path: "The location of the input data."
output_path: "The location of the output file."
device_target: 'Target device type'
enable_profiling: 'Whether enable profiling while training, default: False'
dataset: "Dataset to be trained and evaluated, choice: ['ml-1m', 'ml-20m']"
train_epochs: "The number of epochs used to train."
batch_size: "Batch size for training and evaluation"
eval_batch_size: "The batch size used for evaluation."
num_neg: "The Number of negative instances to pair with a positive instance."
layers: "The sizes of hidden layers for MLP"
num_factors: "The Embedding size of MF model."
checkpoint_path: "The location of the checkpoint file."
eval_file_name: "Eval output file."
checkpoint_file_path: "The location of the checkpoint file."
pre_result_path: "saving dataset to numpy format path."
post_result_path: "inference result path."
